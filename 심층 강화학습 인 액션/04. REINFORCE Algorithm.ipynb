{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cff9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9bd9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N=50):\n",
    "    kernel = np.ones(N)\n",
    "    conv_len = x.shape[0]-N\n",
    "    y = np.zeros(conv_len)\n",
    "    for i in range(conv_len):\n",
    "        y[i] = kernel @ x[i:i+N]\n",
    "        y[i] /= N\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d5192b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3feeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 4\n",
    "l2 = 150\n",
    "l3 = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(l1, l2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(l2, l3),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "learning_rate = 0.0009\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da2ff57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chami\\anaconda3\\envs\\RL_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "state1 = env.reset()\n",
    "pred = model(torch.from_numpy(state1).float())\n",
    "action = np.random.choice(np.array([0, 1]), p=pred.data.numpy())\n",
    "state2, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82587247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    lenr = len(rewards)\n",
    "    disc_return = torch.pow(gamma,torch.arange(lenr).float()) * rewards\n",
    "    disc_return /= disc_return.max()\n",
    "    return disc_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea2d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, r):\n",
    "    return -1 * torch.sum(r * torch.log(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34e11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DUR = 200\n",
    "MAX_EPISODES = 500\n",
    "gamma = 0.99\n",
    "score = []\n",
    "expectation = 0.0\n",
    "for episode in range(MAX_EPISODES):\n",
    "    curr_state = env.reset()\n",
    "    done = False\n",
    "    transitions = []\n",
    "    \n",
    "    for t in range(MAX_DUR):\n",
    "        act_prob = model(torch.from_numpy(curr_state).float())\n",
    "        action = np.random.choice(np.array([0,1]), p=act_prob.data.numpy())\n",
    "        prev_state = curr_state\n",
    "        curr_state, _, done, info = env.step(action)\n",
    "        transitions.append((prev_state, action, t+1))\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    ep_len = len(transitions)\n",
    "    score.append(ep_len)\n",
    "    reward_batch = torch.Tensor([r for (s,a,r) in transitions]).flip(dims=(0,))\n",
    "    disc_returns = discount_rewards(reward_batch)\n",
    "    state_batch = torch.Tensor([s for (s,a,r) in transitions])\n",
    "    action_batch = torch.Tensor([a for (s,a,r) in transitions])\n",
    "    pred_batch = model(state_batch)\n",
    "    prob_batch = pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "    loss = loss_fn(prob_batch, disc_returns)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8640d444",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.array(score)\n",
    "avg_score = running_mean(score, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568adb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.ylabel(\"Episode Duration\",fontsize=22)\n",
    "plt.xlabel(\"Training Epochs\",fontsize=22)\n",
    "plt.plot(avg_score, color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97abe459",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "games = 100\n",
    "done = False\n",
    "state1 = env.reset()\n",
    "for i in range(games):\n",
    "    t=0\n",
    "    while not done: #F\n",
    "        pred = model(torch.from_numpy(state1).float()) #G\n",
    "        action = np.random.choice(np.array([0,1]), p=pred.data.numpy()) #H\n",
    "        state2, reward, done, info = env.step(action) #I\n",
    "        state1 = state2 \n",
    "        t += 1\n",
    "        if t > MAX_DUR: #L\n",
    "            break;\n",
    "    state1 = env.reset()\n",
    "    done = False\n",
    "    score.append(t)\n",
    "score = np.array(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb685504",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(score.shape[0]),score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
