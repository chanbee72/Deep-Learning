{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4ec85f",
   "metadata": {},
   "source": [
    "# DRQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7851467",
   "metadata": {},
   "source": [
    "### 기본적인 패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e1290d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import tensorflow.contrib.slim as slim\n",
    "%matplotlib inline\n",
    "\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9448ae",
   "metadata": {},
   "source": [
    "### 환경 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bc0788f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM00lEQVR4nO3db4wc9X3H8fenNoSEtLEN1HIx7RkFgVBVIDoREH2QQmgJjSAPohSUVFFFlSc0hTZSYtpHVKqUSFUSHlSVECRFVcqfOLSxUJTUdaiqSq2DiWkLGIJJTLBlsBEQ0j5AdfLtgx2nB/Xdzt3u3u3ye7+k0+7M7N38Zkcfz+zs+PtNVSHpre/n1noAklaHYZcaYdilRhh2qRGGXWqEYZcaMVLYk1yT5OkkB5JsH9egJI1fVvo9e5J1wPeAq4FDwCPAjVX15PiGJ2lc1o/wu5cCB6rq+wBJ7gOuBxYN+5lnnllzc3MjrFLSUg4ePMhLL72Uky0bJexnA88vmD4EvHepX5ibm2Pv3r0jrFLSUubn5xddNvELdEk+kWRvkr3Hjh2b9OokLWKUsB8GzlkwvbWb9wZVdWdVzVfV/FlnnTXC6iSNYpSwPwKcl2RbklOBG4Cd4xmWpHFb8Wf2qjqe5A+AbwHrgC9V1RNjG5mksRrlAh1V9Q3gG2Mai6QJ8g46qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRFDw57kS0mOJnl8wbxNSXYleaZ73DjZYUoaVZ8j+18D17xp3nZgd1WdB+zupiVNsaFhr6p/Bl5+0+zrgXu65/cAHxrvsCSN20o/s2+uqiPd8xeAzWMaj6QJGfkCXQ06Qy7aHdKOMNJ0WGnYX0yyBaB7PLrYC+0II02HlYZ9J/Dx7vnHga+PZziSJqXPV2/3Av8KnJ/kUJKbgM8CVyd5Bnh/Ny1pig3tCFNVNy6y6Koxj0XSBHkHndQIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9SIPmWpzknycJInkzyR5JZuvl1hpBnS58h+HPhUVV0IXAbcnORC7AojzZQ+HWGOVNV3u+c/BvYDZ2NXGGmmLOsze5I54BJgDz27wtgkQpoOvcOe5J3A14Bbq+q1hcuW6gpjkwhpOvQKe5JTGAT9K1X1YDe7d1cYSWuvz9X4AHcD+6vq8wsW2RVGmiFDm0QAVwC/C/xnkse6eX/CoAvMA12HmOeAj0xkhJLGok9HmH8Bsshiu8JIM8I76KRGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapERlUlFqllSWrt7KZ5Vu0tMX+t7VOqKqTvkke2aVGGHapEX1q0J2W5DtJ/r3rCHN7N39bkj1JDiS5P8mpkx+upJXqc2R/Hbiyqi4CLgauSXIZ8DngC1X1buAV4KaJjVLSyPp0hKmq+q9u8pTup4ArgR3dfDvCSFOub934dV1l2aPALuBZ4NWqOt695BCDllAn+92fdYQZw3glrVCvsFfVT6rqYmArcClwQd8VLOwIs7IhShqHZV2Nr6pXgYeBy4ENSU6Uot4KHB7v0CSNU5+r8Wcl2dA9fztwNYNOrg8DH+5eZkcYacoNvYMuya8xuAC3jsE/Dg9U1Z8lORe4D9gE7AM+VlWvD/lb3h42lG/R0ryDbpjF7qDzdtmp41u0NMM+jLfLSo0z7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS43oHfaunPS+JA9103aEkWbIco7stzAoNHmCHWGkGdK3ScRW4LeBu7rpYEcYaab0PbJ/Efg08NNu+gzsCCPNlD514z8IHK2qR1eyAjvCSNNh/fCXcAVwXZJrgdOAXwDuoOsI0x3d7QgjTbk+XVxvq6qtVTUH3AB8u6o+ih1hpJkyyvfsnwH+OMkBBp/h7x7PkCRNgh1hpo5v0dLsCDOMHWGkxhl2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRvQpOEmSg8CPgZ8Ax6tqPskm4H5gDjgIfKSqXpnMMCWNajlH9t+oqosXlITeDuyuqvOA3d20pCk1ymn89Qw6wYAdYaSp1zfsBfxDkkeTfKKbt7mqjnTPXwA2n+wX7QgjTYde1WWTnF1Vh5P8IrAL+CSws6o2LHjNK1W1ccjfsXTqUL5FS7O67DAjVZetqsPd41Hg74BLgReTbAHoHo+OZ6iSJqFPr7fTk/z8iefAbwKPAzsZdIIBO8JIU2/oaXyScxkczWHwVd3fVtWfJzkDeAD4ZeA5Bl+9vTzkb3mOOpRv0dI8jR9msdN4O8JMHd+ipRn2YewIIzXOsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71IhexSu0mrxpRJPhkV1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRvcKeZEOSHUmeSrI/yeVJNiXZleSZ7nHJyrKS1lbfI/sdwDer6gLgImA/doSRZkqfgpPvAh4Dzq0FL07yNPC+qjrSlZL+p6o6f8jfssCaNGGj1KDbBhwDvpxkX5K7upLSdoSRZkifI/s88G/AFVW1J8kdwGvAJ+0II02fUY7sh4BDVbWnm94BvAc7wkgzZWjYq+oF4PkkJz6PXwU8iR1hpJnSt7HjxcBdwKnA94HfY/APhR1hpCljRxipEXaEkRpn2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdaoRhlxph2KVGGHapEYZdasTQsCc5P8ljC35eS3KrTSKk2bKsSjVJ1gGHgfcCNwMvV9Vnk2wHNlbVZ4b8vpVqpAkbV6Waq4Bnq+o54Hrgnm7+PcCHVjw6SRO33LDfANzbPe/VJELSdOgd9iSnAtcBX33zsq4t1ElP0e0II02H5RzZPwB8t6pe7KZ7NYmoqjurar6q5kcbqqRRLCfsN/J/p/BgkwhppvRtEnE68EMGnVx/1M07A5tESFPHJhFSI2wSITXOsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjWiV9iT/FGSJ5I8nuTeJKcl2ZZkT5IDSe7vqs9KmlJ92j+dDfwhMF9VvwqsY1A//nPAF6rq3cArwE2THKik0fQ9jV8PvD3JeuAdwBHgSmBHt9yOMNKUGxr2qjoM/AWD6rJHgB8BjwKvVtXx7mWHgLMnNUhJo+tzGr+RQV+3bcAvAacD1/RdgR1hpOmwvsdr3g/8oKqOASR5ELgC2JBkfXd038qgu+v/U1V3And2v2spaWmN9PnM/kPgsiTvSBIGnVyfBB4GPty9xo4w0pTr2xHmduB3gOPAPuD3GXxGvw/Y1M37WFW9PuTveGSXJsyOMFIj7AgjNc6wS40w7FIjDLvUiD7fs4/TS8B/d49vFWfi9kyrt9K2QL/t+ZXFFqzq1XiAJHuran5VVzpBbs/0eittC4y+PZ7GS40w7FIj1iLsd67BOifJ7Zleb6VtgRG3Z9U/s0taG57GS41Y1bAnuSbJ013duu2rue5RJTknycNJnuzq8d3Szd+UZFeSZ7rHjWs91uVIsi7JviQPddMzW1swyYYkO5I8lWR/kstnef+Mu/bjqoU9yTrgL4EPABcCNya5cLXWPwbHgU9V1YXAZcDN3fi3A7ur6jxgdzc9S24B9i+YnuXagncA36yqC4CLGGzXTO6fidR+rKpV+QEuB761YPo24LbVWv8EtufrwNXA08CWbt4W4Om1HtsytmErgwBcCTwEhMFNG+tPts+m+Qd4F/ADuutQC+bP5P5h8F/In2fwX8jXd/vnt0bZP6t5Gn9i8CfMbN26JHPAJcAeYHNVHekWvQBsXqtxrcAXgU8DP+2mz2B2awtuA44BX+4+ltyV5HRmdP/UBGo/eoFumZK8E/gacGtVvbZwWQ3+uZ2JrzeSfBA4WlWPrvVYxmQ98B7gr6rqEga3Zb/hlH3G9s9ItR9PZjXDfhg4Z8H0onXrplWSUxgE/StV9WA3+8UkW7rlW4CjazW+ZboCuC7JQQYVh65k8Jl3Q1cyHGZrHx0CDlXVnm56B4Pwz+r++Vntx6r6H+ANtR+71yxr/6xm2B8BzuuuJp7K4GLDzlVc/0i6+nt3A/ur6vMLFu1kUIMPZqgWX1XdVlVbq2qOwb74dlV9lBmtLVhVLwDPJzm/m3WiVuJM7h8mUftxlS86XAt8D3gW+NO1vgiyzLH/OoNTwP8AHut+rmXwOXc38Azwj8CmtR7rCrbtfcBD3fNzge8AB4CvAm9b6/EtYzsuBvZ2++jvgY2zvH+A24GngMeBvwHeNsr+8Q46qRFeoJMaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWrE/wIebfxlUggbZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "# partial : 부분 관찰 여부, size : gridworld의 크기\n",
    "env = gameEnv(partial=False, size=9) # 크기 9의 완전 관찰 MDP\n",
    "env = gameEnv(partial=True, size=9)  # 크기 9의 부분 관찰 MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00323569",
   "metadata": {},
   "source": [
    "### 네트워크 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b1abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnetwork():\n",
    "    # 초기화 부분\n",
    "    def __init__(self, h_size, rnn_cell, myScope):\n",
    "        # scalarInput 변수 설정 ( 게임에서의 하나의 프레임  21168 = 84 * 84 * 3 )\n",
    "        self.scalarInput = tf.placeholder(shape=[None, 21168], dtype=tf.float32)\n",
    "        # scalarInput을 배열로 만듦 ( 프레임 -> 배열 )\n",
    "        self.imageIn = tf.reshape(self.scalarInput, shape=[-1,84,84,3])\n",
    "        # 합성곱 계층 설정\n",
    "        self.conv1 = slim.convolution2d(inputs=self.imageIn, num_outputs=32, kernel_size=[8,8], stride=[4,4], padding='VALID', biases_initializer=None, scope=myScope+'_conv1')\n",
    "        self.conv2 = slim.convolution2d(inputs=self.conv1, num_outputs=64, kernel_size=[4,4], stride=[2,2], padding='VALID', biases_initializer=None, scope=myScope+'_conv2')\n",
    "        self.conv3 = slim.convolution2d(inputs=self.conv2, num_outputs=64, kernel_size=[3,3], stride=[1,1], padding='VALID', biases_initializer=None, scope=myScope+'_conv3')\n",
    "        self.conv4 = slim.convolution2d(inputs=self.conv3, num_outputs=h_size, kernel_size=[7,7], stride=[1,1], padding='VALID', biases_initializer=None, scope=myScope+'_conv4')\n",
    "        \n",
    "        # trainLength 변수 설정 ( 학습 길이 )\n",
    "        self.trainLength = tf.placeholder(dtype=tf.int32)\n",
    "        \n",
    "        # 순환 신경망 구현\n",
    "        # batch size 설정\n",
    "        self.batch_size = tf.placeholder(dtype=tf.int32, shape=[])\n",
    "        # RNN 처리를 위한 크기 조정 ( batch * trace * units )\n",
    "        self.convFlat = tf.reshape(slim.flatten(self.conv4), [self.batch_size, self.trainLength, h_size])\n",
    "        # RNN 초기 상태\n",
    "        self.state_in = cell.zero_state(self.batch_size, tf.float32)\n",
    "        # RNN 처리\n",
    "        self.rnn, self.rnn_state = tf.nn.dynamic_rnn(inputs=self.convFlat, cell=rnn_cell, dtype=tf.float32, initial_state=self.state_in, scope=myScope+'_rnn')\n",
    "        # 다음 계층을 위한 크기 조정\n",
    "        self.rnn = tf.reshape(self.rnn, shape=[-1,h_size])\n",
    "\n",
    "        # 듀얼링 DQN 구현\n",
    "        # RNN 결과를 어드밴티지 스트림과 가치 스트림으로 분리\n",
    "        self.streamA, self.streamV = tf.split(self.rnn, 2, 1)\n",
    "        # AW, VW 변수 설정\n",
    "        self.AW = tf.Variable(tf.random_normal([h_size//2, 4]))\n",
    "        self.VW = tf.Variable(tf.random_normal([h_size//2, 1]))\n",
    "        # 어드밴티지 스트림과 AW의 행렬곱으로 어드밴티지 계산\n",
    "        self.Advantage = tf.matmul(self.streamA, self.AW)\n",
    "        # 가치 스트림과 VW의 행렬곱으로 가치 계산\n",
    "        self.Value = tf.matmul(self.streamV, self.VW)\n",
    "        # 어드밴티지와 입력값의 그래디언트값으로 gif 생성 시 불투명도의 여부 결정\n",
    "        self.salience = tf.gradients(self.Advantage, self.imageIn)\n",
    "        \n",
    "        # 가치 함수와 어드밴티지 함수의 조합으로 Q값 계산 ( Q = V + ( A - avg(A) ) )\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage, tf.reduce_mean(self.Advantage, axis=1, keep_dims=True))\n",
    "        # Q 값을 통해 행동 예측\n",
    "        self.predict = tf.argmax(self.Qout, 1)\n",
    "        \n",
    "        # 타깃 Q 변수 설정, 액션 변수 설정, 액션의 원핫 벡터 생성\n",
    "        self.targetQ = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, 4, dtype=tf.float32)\n",
    "        \n",
    "        # 예측 Q 계산\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        # 타깃 Q와 예측 Q의 차의 제곱합으로 오차를 구함\n",
    "        self.td_error = tf.square(self.targetQ - self.Q)\n",
    "        \n",
    "        # 오차의 절반을 마스크 처리\n",
    "        self.maskA = tf.zeros([self.batch_size, self.trainLength//2])\n",
    "        self.maskB = tf.ones([self.batch_size, self.trainLength//2])\n",
    "        self.mask = tf.concat([self.maskA, self.maskB], 1)\n",
    "        self.mask = tf.reshape(self.mask, [-1])\n",
    "        self.loss = tf.reduce_mean(self.td_erroe*self.mask)\n",
    "        \n",
    "        # 최적화 설정 ( AdamOptimizer, 손실 최소화 ) \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45220b",
   "metadata": {},
   "source": [
    "### 경험 리플레이\n",
    "버퍼에 경험을 저장해두었다가 랜덤하게 뽑아서 네트워크 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80ed34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experience_buffer():\n",
    "    # 경험 버퍼 초기화\n",
    "    def __init__(self, buffer_size = 1000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    # 버퍼에 경험 추가\n",
    "    def add(self, experience):\n",
    "        # buffer size를 넘으면 앞에서부터 지우고 추가\n",
    "        if len(self.buffer) + 1 >= self.buffer_size:\n",
    "            self.buffer[0:(1+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    # 버퍼에서 랜덤하게 경험 추출\n",
    "    def sample(self, batch_size, trace_length):\n",
    "        # 버퍼에서 batch size만큼 샘플 추출\n",
    "        sampled_episodes = random.sample(self.buffer, batch_size)\n",
    "        sampledTraces = []\n",
    "        for episode in sampled_episodes:\n",
    "            point = np.random.randint(0, len(list(episode)) + 1 - trace_length)\n",
    "            sampledTraces.append(episode[point:point+trace_length])\n",
    "        sampledTraces = np.array(sampledTraces)\n",
    "        return np.reshape(sampledTraces, [batch_size*trace_length, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e64741",
   "metadata": {},
   "source": [
    "### 네트워크 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27691821",
   "metadata": {},
   "source": [
    "학습 매개변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a29a7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4            # 각 학습 단계에서 사용할 경험 배치의 수\n",
    "trace_length = 8          # 학습 시 각 경험 추이의 길이\n",
    "update_freq = 5           # 학습 단계 업데이트 빈도\n",
    "y = .99                   # 타깃 Q 값에 대한 할인 계수\n",
    "startE = 1                # 랜덤한 액션을 시작할 가능성\n",
    "endE = 0.1                # 랜덤한 액션을 끝낼 가능성\n",
    "anneling_steps = 10000    # startE에서 endE로 줄어드는 데 필요한 학습 단계 수\n",
    "num_episodes = 10000      # 네트워크를 학습시킬 에피소드의 수\n",
    "pre_train_steps = 10000   # 학습 시작 전 랜덤 액션의 단계 수\n",
    "load_model = False       # 저장된 모델을 로드할 지 여부\n",
    "path = \"./drqn\"           # 모델을 저장할 경로\n",
    "h_size = 512              # 어드밴티지 / 가치 스트림으로 분리되기 전 마지막 합성곱 계층의 크기\n",
    "max_epLength = 50         # 허용되는 최대 에피소드 길이\n",
    "time_per_step = 1         # gif 생성에 사용되는 각 단계의 길이\n",
    "summaryLength = 100       # 분석을 위해 주기적으로 저장할 에피소드의 수\n",
    "tau = 0.001               # 타깃 네트워크를 제 1 네트워크로 업데이트하는 비율"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5684250f",
   "metadata": {},
   "source": [
    "학습 환경 설절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf834fa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 텐서플로 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 최초의 Q 네트워크와 타깃 Q 네트워크에 대한 RNN 셀 정의\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "mainQN = Qnetwork(h_size, cell, 'main')\n",
    "targetQN = Qnetwork(h_size, cellT, 'target')\n",
    "\n",
    "\n",
    "# 텐서플로 전역 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "# 모델을 저장할 saver 정의\n",
    "saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "\n",
    "# 학습시킬 변수 선언\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "\n",
    "# 타깃 Q 네트워크 업데이트하기 위한 값 설정\n",
    "targetOps = updateTargetGraph(trainables, tau)\n",
    "\n",
    "\n",
    "# 경험 버퍼 선언\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "# 랜덤 액션이 감소하는 비율 설정\n",
    "e = startE\n",
    "stepDrop = (startE - endE)/anneling_steps\n",
    "\n",
    "# 에피소드별 단계 수 리스트\n",
    "jList = []\n",
    "# 보상의 총계 리스트\n",
    "rList = []\n",
    "# 총 단계 수\n",
    "total_steps = 0\n",
    "\n",
    "# path에 해당하는 경로가 없다면 디렉토리 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# 로그 파일의 첫 번째 행 작성\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode', 'Length', 'Reward', 'IMG', 'LOG', 'SAL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1899b6",
   "metadata": {},
   "source": [
    "실제 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # path에 저장된 모델 로드\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 타깃 네트워크가 제 1 네트워크와 동일하도록 설정\n",
    "    updateTarget(targetOps, sess)\n",
    "    \n",
    "    # 에피소드 수만큼 학습 진행\n",
    "    for i in range(num_episodes):\n",
    "        # 경험 버퍼 초기화\n",
    "        episodeBuffer = []\n",
    "        \n",
    "        # 환경 초기화\n",
    "        sP = env.reset()\n",
    "        # 새로운 관찰\n",
    "        s = processState(sP)\n",
    "        # 게임 종료 여부\n",
    "        d = False\n",
    "        # 보상의 합계\n",
    "        rAll = 0\n",
    "        # 단계 수\n",
    "        j = 0\n",
    "        \n",
    "        # 순환 계층의 은닉상태 초기화\n",
    "        state = (np.zeros([1,h_size]), np.zeros([1, h_size]))\n",
    "        \n",
    "        # 최대 에피소드 길이만큼 단계 진행 ( 최대 50회 시도 )\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            \n",
    "            # e의 확률로 랜덤 액션 선택 ( greedy )\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                state1 = sess.run(mainQN.rnn_state, feed_dict={mainQN.scalarInput:[s/255.0], mainQN.trainLength:1, \n",
    "                                                               mainQN.state_in:state, mainQN.batch_size:1})\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict, mainQN.rnn_state], \n",
    "                                     feed_dict={mainQN.scalarInput:[s/255.0], mainQN.trainLength:1, \n",
    "                                                mainQN.state_in:state, mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "                \n",
    "            # 선택된 액션 진행 ( 상태, 보상, 종료 여부 )\n",
    "            s1P, r, d = env.step(a)\n",
    "            # 상태에 따른 관찰\n",
    "            s1 = processState(s1P)\n",
    "            # 총 단계 증가\n",
    "            total_steps += 1\n",
    "            # 에피소드 버퍼에 에피소드 추가 ( 상태, 액션, 보상, 다음 상태, 종료 여부 )\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]), [1,5]))\n",
    "            \n",
    "            # 총 단계 수가 랜덤 액션 단계수보다 크면 랜덤 액션 가능성 조절 및 네트워크 업데이트\n",
    "            if total_steps > pre_train_steps:\n",
    "                # 랜덤 액션 가능성 조절\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                # 설정한 빈도수에 맞게 네트워크 업데이트\n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    # 타깃 네트워크 업데이트\n",
    "                    updateTarget(targetOps, sess)\n",
    "                    # 순환 계층의 은닉상태 초기화\n",
    "                    state_train = (np.zeros([batch_size, h_size]), np.zeros([batch_size, h_size]))\n",
    "                    # 경험에서 랜덤한 배치 샘플링\n",
    "                    trainBatch = myBuffer.sample(batch_size, trace_length)\n",
    "                    # 타깃 Q 갑셍 대해 더블 DQN 업데이트 수행\n",
    "                    # 액션 예측 값\n",
    "                    Q1 = sess.run(mainQN.predict, \n",
    "                                  feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,3]/255.0), \n",
    "                                             mainQN.trainLength:trace_length, \n",
    "                                             mainQN.state_in:state_train, mainQN.batch_size:batch_size})\n",
    "                    # 타깃 네트워크의 Q 값\n",
    "                    Q2 = sess.run(targetQN.Qout, \n",
    "                                  feed_dict={targetQN.scalarInput:np.vstack(trainBatch[:,3]/255.0), \n",
    "                                             targetQN.trainLength:trace_length, \n",
    "                                             targetQN.state_in:state_train, targetQN.batch_size:batch_size})\n",
    "                    # 종료 여부에 따른 라벨\n",
    "                    end_multiplier = -(trainBatch[:,4] - 1)\n",
    "                    # 더블 Q 값 구하기\n",
    "                    doubleQ = Q2[range(batch_size*trace_length), Q1]\n",
    "                    # 타깃 Q 값 구하기\n",
    "                    targetQ = trainBatch[:,2] + (y*doubleQ*end_multiplier)\n",
    "                    \n",
    "                    # 제 1 네트워크 업데이트\n",
    "                    sess.run(mainQN.updateModel, \n",
    "                             feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]/255.0), mainQN.targetQ:targetQ, \n",
    "                                        mainQN.actions:trainBatch[:,1], mainQN.trainLength:trace_length, \n",
    "                                        mainQN.state_in:state_train, mainQN.batch_size:batch_size})\n",
    "            # 총 보상 업데이트\n",
    "            rAll += r\n",
    "            # 상태 전환 ( 다음 상태로 이동 )\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            \n",
    "            if d == True:\n",
    "                break\n",
    "        \n",
    "        # 경험 버퍼에 에피소드 추가\n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        episodeBuffer = list(zip(bufferArray))\n",
    "        myBuffer.add(episodeBuffer)\n",
    "        # 보상 총계와 단계 수 업데이트\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        # 주기적으로 모델 저장\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            saver.save(sess, path+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print(total_steps, np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(i, rList, jList, np.reshape(np.array(episodeBuffer), [len(episodeBuffer), 5]), \n",
    "                         summaryLength, h_size, sess, mainQN, time_per_step)\n",
    "    saver.save(sess, path+'/model-'+str(i)+'.cptk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fee6467",
   "metadata": {},
   "source": [
    "### 네트워크 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2631cdaf",
   "metadata": {},
   "source": [
    "테스트 매개변수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6872c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 0.01               # 랜덤 액션 선택 가능성\n",
    "num_episodes = 10000   # 에피소드 개수\n",
    "load_model = True     # 저장한 모델 로드 여부\n",
    "path = \"./drqn\"       # 로드 / 저장할 모델 경로\n",
    "h_size = 512          # 은닉계층 크기\n",
    "max_epLength = 50     # 최대 에피소드 길이\n",
    "time_per_step = 1     # 각 단계의 길이 ( gif 생성 시 사용 )\n",
    "summaryLength = 100   # 분석을 위해 주기적으로 저장할 에피소드의 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0e05c",
   "metadata": {},
   "source": [
    "테스트 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f057ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로 그래프 초기화\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 최초의 Q 네트워크와 타깃 Q 네트워크에 대한 RNN 셀 정의\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "cellT = tf.contrib.rnn.BasicLSTMCell(num_units=h_size, state_is_tuple=True)\n",
    "\n",
    "# 제 1 네트워크와 타깃 네트워크 생성\n",
    "mainQN = Qnetwork(h_size, cell 'main')\n",
    "targetQN = Qnetwork(h_size, cellT, 'target')\n",
    "\n",
    "# 텐서플로 전역 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 모델을 저장할 saver 정의\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "# 에피소드별 단계 수 리스트\n",
    "jList = []\n",
    "# 보상의 총계 리스트\n",
    "rList = []\n",
    "# 총 단계 수\n",
    "total_steps = 0\n",
    "\n",
    "# path에 해당하는 경로가 없다면 디렉토리 생성\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "# 로그 파일의 첫 번째 행 작성\n",
    "with open('./Center/log.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(['Episode', 'Length', 'Reward', 'IMG', 'LOG', 'SAL'])\n",
    "    \n",
    "# 테스트 실행\n",
    "with tf.Session() as sess:\n",
    "    # path에 저장된 모델 로드\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        sess.run(init)\n",
    "        \n",
    "    # 에피소드 수만큼 테스트 진행\n",
    "    for i in range(num_episodes):\n",
    "        # 경험 버퍼 초기화\n",
    "        episodeBuffer = []\n",
    "        \n",
    "        # 환경 초기화\n",
    "        sP = env.reset()\n",
    "        # 새로운 관찰\n",
    "        s = processState(sP)\n",
    "        # 게임 종료 여부\n",
    "        d = False\n",
    "        # 보상의 합계\n",
    "        rAll = 0\n",
    "        # 단계 수\n",
    "        j = 0\n",
    "        \n",
    "        # 순환 계층의 은닉상태 초기화\n",
    "        state = (np.zeros([1, h_size]), np.zeros([1, h_size]))\n",
    "        \n",
    "        # 최대 에피소드 길이만큼 단계 진행 ( 최대 50회 시도 )\n",
    "        while j < max_epLength:\n",
    "            j += 1\n",
    "            \n",
    "            # e의 확률로 랜덤 액션 선택\n",
    "            if np.random.rand(1) < e:\n",
    "                state1 = sess.run(mainQN.rnn_state, feed_dict={mainQN.scalarInput:[s/255.0], mainQN.trainLength:1, \n",
    "                                                              mainQN.state_in:state, mainQN.batch_size:1})\n",
    "                a = np.random.randint(0, 4)\n",
    "            else:\n",
    "                a, state1 = sess.run([mainQN.predict, mainQN.rnn_state], feed_dict={mainQN.scalarInput:[s/255.0], \n",
    "                                                                                    mainQN.trainLength:1, \n",
    "                                                                                    mainQN.state_in:state, mainQN.batch_size:1})\n",
    "                a = a[0]\n",
    "            \n",
    "            # 선택된 액션 진행 ( 상태, 보상, 종료 여부 )\n",
    "            s1P, r, d = env.step(a)\n",
    "            # 상태에 따른 관찰\n",
    "            s1 = processState(s1P)\n",
    "            # 총 단계 증가\n",
    "            total_steps += 1\n",
    "            \n",
    "            # 에피소드 버퍼에 에피소드 추가 ( 상태, 액션, 보상, 다음 상태, 종료 여부 )\n",
    "            episodeBuffer.append(np.reshape(np.array([s,a,r,s1,d]),[1,5]))\n",
    "            \n",
    "            # 총 보상 업데이트\n",
    "            rAll += r\n",
    "            # 상태 전환 ( 다음 상태로 이동 )\n",
    "            s = s1\n",
    "            sP = s1P\n",
    "            state = state1\n",
    "            if d == True:\n",
    "                break\n",
    "                \n",
    "        bufferArray = np.array(episodeBuffer)\n",
    "        # 보상 총계와 단계 수 업데이트\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        \n",
    "        # 주기적으로 모델 저장\n",
    "        if len(rList) % summaryLength == 0 and len(rList) != 0:\n",
    "            print(total_steps, np.mean(rList[-summaryLength:]), e)\n",
    "            saveToCenter(i, rList, jList, np.reshape(np.array(episodeBuffer), [len(episodeBuffer),5]), \n",
    "                         summaryLength, h_size, sess, mainQN, time_per_step)\n",
    "            \n",
    "print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
